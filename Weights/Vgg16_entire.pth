{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vgg16_entire.pth","provenance":[],"authorship_tag":"ABX9TyOWSjSeyL3BuMvjpNPwExxP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eIIYYpEjSxdw","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scikitplot as skplt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6B5rttzUjo7","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gds4c0TWUqnR","colab_type":"code","colab":{}},"source":["data_dir= '/content/gdrive/\"My Drive\"/Assignment_5_Dataset'\n","!ls {data_dir}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTnO5Qs7UVsN","colab_type":"code","colab":{}},"source":["#Define transforms for the training data and testing data\n","train_transforms = transforms.Compose([transforms.RandomRotation(0),\n","                                       transforms.RandomResizedCrop(224),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.485, 0.456, 0.406],\n","                                                            [0.229, 0.224, 0.225])])\n","\n","test_transforms = transforms.Compose([transforms.Resize(256),\n","                                      transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.485, 0.456, 0.406],\n","                                                           [0.229, 0.224, 0.225])])\n","\n","val_transforms = transforms.Compose([transforms.Resize(256),\n","                                      transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.485, 0.456, 0.406],\n","                                                           [0.229, 0.224, 0.225])])\n","\n","#pass transform here-in\n","#1train_data = datasets.ImageFolder(root= \"/content/gdrive/My Drive/Assignment_5_Dataset/train\", transform=train_transforms)\n","#1test_data = datasets.ImageFolder(root= \"/content/gdrive/My Drive/Assignment_5_Dataset/test\", transform=test_transforms)\n","#1val_data = datasets.ImageFolder(root= \"/content/gdrive/My Drive/Assignment_5_Dataset/validation\", transform=val_transforms)\n","train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n","test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n","val_data = datasets.ImageFolder(data_dir + '/validation', transform=val_transforms)\n","\n","#data loaders\n","trainloader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n","testloader = torch.utils.data.DataLoader(test_data, batch_size=10, shuffle=True)\n","valloader = torch.utils.data.DataLoader(val_data, batch_size=10, shuffle=True)\n","                                          \n","print(\"Classes: \")\n","class_names = train_data.classes\n","print(class_names)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujxyXIUYUwnX","colab_type":"code","colab":{}},"source":["def imshow(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    plt.axis('off')\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)\n","\n","def show_databatch(inputs, classes):\n","    out = torchvision.utils.make_grid(inputs)\n","    imshow(out, title=[class_names[x] for x in classes])\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(trainloader))\n","show_databatch(inputs, classes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0QHUo_EhU2a0","colab_type":"code","colab":{}},"source":["# Load the pretrained model from pytorch\n","vgg16 = models.vgg16(pretrained=True)\n","print(vgg16)\n","print('Output Layer of VGG16 : ', vgg16.classifier[6].out_features) # 1000 "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PefhUPwtXAVK","colab_type":"text"},"source":["Training Part"]},{"cell_type":"code","metadata":{"id":"aJ8d9UqgW-8G","colab_type":"code","colab":{}},"source":["Epochs = 10\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7KnJKdsVROh","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","\n","#if you have gpu then you need to convert the network and data to cuda\n","#the easiest way is to first check for device and then convert network and data to device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","vgg16.to(device)\n","\n","vgg16.train()\n","\n","for epoch in range(Epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    pbar = tqdm(enumerate(trainloader))\n","    for i, data in pbar:\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation \n","        # because PyTorch accumulates the gradients on subsequent backward passes. \n","        # This is convenient while training RNNs. \n","        # So, the default action is to accumulate the gradients on every loss.backward() call\n","\n","        # forward + backward + optimize\n","        outputs = vgg16(inputs)               #----> forward pass\n","        loss = criterion(outputs, labels)   #----> compute loss\n","        loss.backward()                     #----> backward pass\n","        optimizer.step()                    #----> weights update\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        \n","        pbar.set_description(\n","     from tqdm import tqdm\n","\n","#if you have gpu then you need to convert the network and data to cuda\n","#the easiest way is to first check for device and then convert network and data to device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","vgg16.to(device)\n","\n","vgg16.train()\n","\n","for epoch in range(Epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    pbar = tqdm(enumerate(trainloader))\n","    for i, data in pbar:\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation \n","        # because PyTorch accumulates the gradients on subsequent backward passes. \n","        # This is convenient while training RNNs. \n","        # So, the default action is to accumulate the gradients on every loss.backward() call\n","\n","        # forward + backward + optimize\n","        outputs = vgg16(inputs)               #----> forward pass\n","        loss = criterion(outputs, labels)   #----> compute loss\n","        loss.backward()                     #----> backward pass\n","        optimizer.step()                    #----> weights update\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        \n","        pbar.set_description(\n","            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, i * len(inputs), len(trainloader.dataset),\n","                100. * i / len(trainloader),\n","                loss.data))\n","\n","        \n","    torch.save(vgg16.state_dict(), 'vgg16_ft.pth')\n","\n","print('Finished Training')\n","#epocharray=range(0,Epochs)\n","plt.plot(epoch,loss.data)\n","plt.ylabel('Training Loss')\n","plt.xlabel('Epochs')\n","    from tqdm import tqdm\n","\n","#if you have gpu then you need to convert the network and data to cuda\n","#the easiest way is to first check for device and then convert network and data to device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","vgg16.to(device)\n","\n","vgg16.train()\n","\n","for epoch in range(Epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    pbar = tqdm(enumerate(trainloader))\n","    for i, data in pbar:\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation \n","        # because PyTorch accumulates the gradients on subsequent backward passes. \n","        # This is convenient while training RNNs. \n","        # So, the default action is to accumulate the gradients on every loss.backward() call\n","\n","        # forward + backward + optimize\n","        outputs = vgg16(inputs)               #----> forward pass\n","        loss = criterion(outputs, labels)   #----> compute loss\n","        loss.backward()                     #----> backward pass\n","        optimizer.step()                    #----> weights update\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        \n","        pbar.set_description(\n","            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, i * len(inputs), len(trainloader.dataset),\n","                100. * i / len(trainloader),\n","                loss.data))\n","\n","        \n","    torch.save(vgg16.state_dict(), 'vgg16_ft.pth')\n","\n","print('Finished Training')\n","#epocharray=range(0,Epochs)\n","plt.plot(epoch,loss.data)\n","plt.ylabel('Training Loss')\n","plt.xlabel('Epochs')\n","           'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, i * len(inputs), len(trainloader.dataset),\n","                100. * i / len(trainloader),\n","                loss.data))\n","\n","        \n","    torch.save(vgg16.state_dict(), 'vgg16_ft.pth')\n","\n","print('Finished Training')\n","#epocharray=range(0,Epochs)\n","plt.plot(epoch,loss.data)\n","plt.ylabel('Training Loss')\n","plt.xlabel('Epochs')\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TrIjhvEVckK","colab_type":"code","colab":{}},"source":["acc=[]\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for k in range(Epochs):\n","        for data in trainloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = vgg16(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","        acc.append(100*correct/total)\n","\n","print('Accuracy of the network on Training images: %d %%' % (\n","    100 * correct / total))\n","epocharray=range(0,Epochs)\n","plt.plot(epocharray,acc)\n","plt.ylabel('Training Accuracy')\n","plt.xlabel('Epochs')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1zW_B2pV5y8","colab_type":"code","colab":{}},"source":["#Confusion Matrix part\n","nb_classes = 2\n","\n","confusion_matrix = torch.zeros(nb_classes, nb_classes)\n","with torch.no_grad():\n","    for i, (inputs, classes) in enumerate(trainloader):\n","        inputs = inputs.to(device)\n","        classes = classes.to(device)\n","        outputs = vgg16(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        for t, p in zip(classes.view(-1), preds.view(-1)):\n","                confusion_matrix[t.long(), p.long()] += 1\n","\n","print(confusion_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vix16xwzV_qz","colab_type":"code","colab":{}},"source":["print(confusion_matrix.diag()/confusion_matrix.sum(1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZhnDYlGWCnY","colab_type":"code","colab":{}},"source":["skplt.metrics.plot_confusion_matrix(classes, preds, figsize=(4,4), normalize=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J16bS4vFXMQ5","colab_type":"text"},"source":["Testing Part"]},{"cell_type":"code","metadata":{"id":"oEb3CvEiVjfi","colab_type":"code","colab":{}},"source":["dataiter = iter(testloader)\n","images, labels = dataiter.next()\n","show_databatch(images, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvCgFA6UVpF8","colab_type":"code","colab":{}},"source":["images, labels = images.to(device), labels.to(device) #-->convert test image to cuda (if available)\n","outputs = vgg16(images)                               #--> forward pass\n","_, predicted = torch.max(outputs, 1)\n","\n","print('Predicted: ', ' '.join('%5s' % class_names[predicted[j]]\n","                              for j in range(len(images))))\n","print('Ground Truth: ', ' '.join('%5s' % class_names[labels[j]]\n","                              for j in range(len(images))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CW9nQpM_V0DD","colab_type":"code","colab":{}},"source":["correct = 0\n","total = 0\n","acc=[]\n","with torch.no_grad():\n","    for k in range(Epochs):\n","        for data in testloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = vgg16(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","        acc.append(100*correct/total)\n","        \n","\n","print('Accuracy of the network on test images: %d %%' % (\n","    100 * correct / total))\n","\n","epocharray=range(0,Epochs)\n","plt.plot(epocharray,acc)\n","plt.ylabel('Test Accuracy')\n","plt.xlabel('Epochs')\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"91xkc79iWbuc","colab_type":"code","colab":{}},"source":["nb_classes = 2\n","\n","confusion_matrix = torch.zeros(nb_classes, nb_classes)\n","with torch.no_grad():\n","    for i, (inputs, classes) in enumerate(testloader):\n","        inputs = inputs.to(device)\n","        classes = classes.to(device)\n","        outputs = vgg16(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        for t, p in zip(classes.view(-1), preds.view(-1)):\n","                confusion_matrix[t.long(), p.long()] += 1\n","\n","print(confusion_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B11uCLqPWTVP","colab_type":"code","colab":{}},"source":["print(confusion_matrix.diag()/confusion_matrix.sum(1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_5tzvv-WZwk","colab_type":"code","colab":{}},"source":["skplt.metrics.plot_confusion_matrix(classes, preds, figsize=(4,4), normalize=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ho9qB5rX8yH","colab_type":"text"},"source":["Validation Part"]},{"cell_type":"code","metadata":{"id":"qnhRMot1WHey","colab_type":"code","colab":{}},"source":["dataiter = iter(valloader)\n","images, labels = dataiter.next()\n","show_databatch(images, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acFlFRzfWLMX","colab_type":"code","colab":{}},"source":["images, labels = images.to(device), labels.to(device) #-->convert test image to cuda (if available)\n","outputs = vgg16(images)                               #--> forward pass\n","_, predicted = torch.max(outputs, 1)\n","\n","print('Predicted: ', ' '.join('%5s' % class_names[predicted[j]]\n","                              for j in range(len(images))))\n","print('Ground Truth: ', ' '.join('%5s' % class_names[labels[j]]\n","                              for j in range(len(images))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-_YsS7hWMlA","colab_type":"code","colab":{}},"source":["acc=[]\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for k in range(Epochs):\n","        for data in valloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = vgg16(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","        acc.append(100*correct/total)\n","\n","print('Accuracy of the network on Validation images: %d %%' % (\n","    100 * correct / total))\n","epocharray=range(0,Epochs)\n","plt.plot(epocharray,acc)\n","plt.ylabel('Validation Accuracy')\n","plt.xlabel('Epochs')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQONBBtOWSJ9","colab_type":"code","colab":{}},"source":["nb_classes = 2\n","\n","confusion_matrix = torch.zeros(nb_classes, nb_classes)\n","with torch.no_grad():\n","    for i, (inputs, classes) in enumerate(valloader):\n","        inputs = inputs.to(device)\n","        classes = classes.to(device)\n","        outputs = vgg16(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        for t, p in zip(classes.view(-1), preds.view(-1)):\n","                confusion_matrix[t.long(), p.long()] += 1\n","\n","print(confusion_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K37TgQCKWetM","colab_type":"code","colab":{}},"source":["print(confusion_matrix.diag()/confusion_matrix.sum(1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eq_p2XnsWkQJ","colab_type":"code","colab":{}},"source":["skplt.metrics.plot_confusion_matrix(classes, preds, figsize=(4,4), normalize=True)\n","plt.show()"],"execution_count":0,"outputs":[]}]}